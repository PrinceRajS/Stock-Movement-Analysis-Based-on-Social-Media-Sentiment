{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2248d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60220120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c11b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication Function\n",
    "def authenticate_reddit(client_id, client_secret, user_agent):\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent,\n",
    "        )\n",
    "        print(\"Authenticated successfully!\")\n",
    "        return reddit\n",
    "    except Exception as e:\n",
    "        print(\"Authentication failed:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b22c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scraping\n",
    "def scrape_reddit_data(reddit, subreddit_name, keyword=None, limit=100):\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts = subreddit.search(keyword, limit=limit) if keyword else subreddit.hot(limit=limit)\n",
    "        data = []\n",
    "\n",
    "        for post in posts:\n",
    "            post.comments.replace_more(limit=0)\n",
    "            comments = \" \".join([comment.body for comment in post.comments.list()])\n",
    "            data.append({\n",
    "                \"Title\": post.title,\n",
    "                \"Comments\": comments,\n",
    "                \"Score\": post.score,\n",
    "                \"Sentiment Text\": post.title + \" \" + comments,\n",
    "                \"Created At\": pd.to_datetime(post.created_utc, unit=\"s\"),\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "    except Exception as e:\n",
    "        print(\"Error during scraping:\", e)\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83265dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def preprocess_text(df):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    df[\"Cleaned Text\"] = df[\"Sentiment Text\"].str.lower()\n",
    "    df[\"Cleaned Text\"] = df[\"Cleaned Text\"].apply(\n",
    "        lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8805de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "def analyze_sentiment(df):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df[\"Sentiment Score\"] = df[\"Cleaned Text\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "884f4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def add_features(df):\n",
    "    df[\"Hour\"] = df[\"Created At\"].dt.hour\n",
    "    df[\"Day\"] = df[\"Created At\"].dt.day_name()\n",
    "    df[\"Comment Length\"] = df[\"Comments\"].str.len()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d675f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train_hybrid_model(df):\n",
    "    # Define target (upward movement if sentiment score > 0.2)\n",
    "    df[\"Stock Movement\"] = (df[\"Sentiment Score\"] > 0.2).astype(int)\n",
    "\n",
    "    # Features and target\n",
    "    features = [\"Sentiment Score\", \"Score\", \"Comment Length\"]\n",
    "    X = df[features]\n",
    "    y = df[\"Stock Movement\"]\n",
    "\n",
    "    # Split and normalize data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Random Forest for feature importance\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    print(\"Random Forest Report:\")\n",
    "    print(classification_report(y_test, rf_model.predict(X_test)))\n",
    "\n",
    "    # LSTM for sequential data\n",
    "    X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "    lstm_model = Sequential([\n",
    "        LSTM(50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "    lstm_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=16, verbose=1)\n",
    "\n",
    "    print(\"LSTM Evaluation:\")\n",
    "    lstm_model.evaluate(X_test_lstm, y_test)\n",
    "\n",
    "    return rf_model, lstm_model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b478103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "def save_models(rf_model, lstm_model, scaler):\n",
    "    joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    lstm_model.save(\"lstm_model.h5\")\n",
    "    print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55a78c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated successfully!\n",
      "Random Forest Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 2s 5ms/step - loss: 0.6891 - accuracy: 0.8113\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6809 - accuracy: 0.8491\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6727 - accuracy: 0.9811\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6648 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6575 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6495 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6417 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6340 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6259 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6181 - accuracy: 1.0000\n",
      "LSTM Evaluation:\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 0.5975 - accuracy: 1.0000\n",
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Reddit API credentials\n",
    "    client_id = \"q9C9_B3Km7S9L4rofTLUvw\"\n",
    "    client_secret = \"fy7yX-94w8usjBQaxspmrbCb_dHD5A\"\n",
    "    user_agent = \"AdditionalCoast770\"\n",
    "\n",
    "    # Authenticating with Reddit\n",
    "    reddit = authenticate_reddit(client_id, client_secret, user_agent)\n",
    "    \n",
    "    # Scrape data from Reddit\n",
    "    data = scrape_reddit_data(reddit, \"StockMarket\", keyword=\"SAIL\", limit=100)\n",
    "    \n",
    "    if not data.empty:\n",
    "        # Preprocess text data\n",
    "        data = preprocess_text(data)\n",
    "        # Perform sentiment analysis\n",
    "        data = analyze_sentiment(data)\n",
    "        # Add features for prediction\n",
    "        data = add_features(data)\n",
    "        # Train hybrid model\n",
    "        rf_model, lstm_model, scaler = train_hybrid_model(data)\n",
    "        # Save trained models and scaler\n",
    "        save_models(rf_model, lstm_model, scaler)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
